{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af666fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mihat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mihat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mihat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2d4cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "df = pd.read_csv( os.path.join(cwd, 'full_dataset_all_labels.csv'))\n",
    "stop_words=set(stopwords.words('english') + list(string.punctuation))\n",
    "stop_words.add('rt') # add word rt (meaning retweet) to stop words\n",
    "df = pd.read_csv('full_dataset_all_labels.csv')\n",
    "#df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef23a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_some_texts(columns, df):\n",
    "    text_idxs = [47, 7240, 7241, 8013, 14500, 16500, 16304, 18300,  21750, 34036, 45159, 71920]\n",
    "    for i in text_idxs:\n",
    "        for column in columns:\n",
    "            print(df[column].iloc[i])\n",
    "#print_some_texts(['text'])\n",
    "\n",
    "def tokenize(text):\n",
    "    #print(text)\n",
    "    text = preprocess_text(text)\n",
    "    #print(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    # Filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation). (adapted from lab example)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in stop_words and len(token) > 2:\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "    \n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)            # remove urls\n",
    "    text = re.sub(\"@[A-Za-z0-9]+\",\"\", text)         # remove twitter handle\n",
    "    text = re.sub(\"&amp;\",\"\", text)                  # &amp; is a special character for ampersand\n",
    "    text = re.sub('<USER>', '', text)               # remove '<USER>' as there are some such strings as user or url is masked with this string\n",
    "    text = re.sub('<URL>', '', text)\n",
    "    text = text.lower() \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)           # Remove punctuations\n",
    "    text = text.lower()                             # Convert to lowercase\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)#remove tags\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)            # remove special characters and digits\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def stemming(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "def lemmatizing(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c8b3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text']=df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5bdb111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['appended'] = df['preprocessed_text']+', this is '+df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da9811c",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b5a1f",
   "metadata": {},
   "source": [
    "## DEFAULT BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1fe12af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb03b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "903a9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "    \n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return np.array(list_token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8baf2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    return pd.Series([tokenized_text, tokens_tensor, segments_tensors], index = ['tokenized_text', 'tokens_tensor', 'segments_tensors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "895f80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label'].unique()\n",
    "dict_labels_len = {}\n",
    "for l in labels:\n",
    "    x = bert_text_preparation(l, tokenizer)\n",
    "    dict_labels_len[l] = len(x[0])-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99f5c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77668/77668 [02:27<00:00, 526.68it/s]\n"
     ]
    }
   ],
   "source": [
    "df[['tokenized_text', 'tokens_tensor', 'segments_tensors']] = df['appended'].progress_apply(bert_text_preparation, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "44572561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['leng'] = df['tokens_tensor'].apply(lambda x: x.size()[1])\n",
    "\n",
    "df = df[df['leng']  <= 512]\n",
    "df.drop(['leng'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "26cc8f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77194/77194 [6:31:33<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "df['bert_emmbeding'] = df.progress_apply(lambda x: get_bert_embeddings(x['tokens_tensor'], x['segments_tensors'], model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e7e56f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77194/77194 [00:09<00:00, 8254.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [abuse]\n",
       "1         [abuse]\n",
       "2         [abuse]\n",
       "3         [abuse]\n",
       "4         [abuse]\n",
       "           ...   \n",
       "77663    [vulgar]\n",
       "77664    [vulgar]\n",
       "77665    [vulgar]\n",
       "77666    [vulgar]\n",
       "77667    [vulgar]\n",
       "Length: 77194, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.progress_apply(lambda x: x['tokenized_text'][- dict_labels_len[x['label']]-1:-1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd1fa99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77194/77194 [01:51<00:00, 692.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>appended</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokens_tensor</th>\n",
       "      <th>segments_tensors</th>\n",
       "      <th>bert_emmbeding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23753</th>\n",
       "      <td>\"\\n\\n Stop your fucking spamming \\n\\nI have to...</td>\n",
       "      <td>insult</td>\n",
       "      <td>stop your fucking spamming i have told you an...</td>\n",
       "      <td>stop your fucking spamming i have told you an...</td>\n",
       "      <td>[[CLS], stop, your, fucking, spa, ##mming, i, ...</td>\n",
       "      <td>[[tensor(101), tensor(2644), tensor(2115), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.14338752627372742, -0.4059569239616394, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   label  \\\n",
       "23753  \"\\n\\n Stop your fucking spamming \\n\\nI have to...  insult   \n",
       "\n",
       "                                       preprocessed_text  \\\n",
       "23753   stop your fucking spamming i have told you an...   \n",
       "\n",
       "                                                appended  \\\n",
       "23753   stop your fucking spamming i have told you an...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "23753  [[CLS], stop, your, fucking, spa, ##mming, i, ...   \n",
       "\n",
       "                                           tokens_tensor  \\\n",
       "23753  [[tensor(101), tensor(2644), tensor(2115), ten...   \n",
       "\n",
       "                                        segments_tensors  \\\n",
       "23753  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "\n",
       "                                          bert_emmbeding  \n",
       "23753  [0.14338752627372742, -0.4059569239616394, -0....  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bert_emmbeding'] = df.progress_apply(lambda x: np.average(x['bert_emmbeding'][- dict_labels_len[x['label']]-1:-1], axis = 0), axis = 1)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92351091",
   "metadata": {},
   "source": [
    "## BERT WITH SENTENCE EMMBEDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cabedf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('stsb-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c07214c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = df['preprocessed_text'].to_numpy()\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66029b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert_sentence_emm'] = sentence_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95508feb",
   "metadata": {},
   "source": [
    "# ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa3c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('first.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a005af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a23ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7faf2724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>appended</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokens_tensor</th>\n",
       "      <th>segments_tensors</th>\n",
       "      <th>bert_emmbeding</th>\n",
       "      <th>bert_sentence_emm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>all trump supporters are worthless pieces of s...</td>\n",
       "      <td>abuse</td>\n",
       "      <td>all trump supporters are worthless pieces of s...</td>\n",
       "      <td>all trump supporters are worthless pieces of s...</td>\n",
       "      <td>[[CLS], all, trump, supporters, are, worthless...</td>\n",
       "      <td>[[tensor(101), tensor(2035), tensor(8398), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.7415121793746948, 0.31940165162086487, 0.24...</td>\n",
       "      <td>[0.04157046601176262, 0.04623369872570038, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37731</th>\n",
       "      <td>Fuck you and Fuck your mom. And her dog.</td>\n",
       "      <td>obscene</td>\n",
       "      <td>fuck you and fuck your mom and her dog</td>\n",
       "      <td>fuck you and fuck your mom and her dog , this ...</td>\n",
       "      <td>[[CLS], fuck, you, and, fuck, your, mom, and, ...</td>\n",
       "      <td>[[tensor(101), tensor(6616), tensor(2017), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.3871220350265503, 0.4998416602611542, 0.015...</td>\n",
       "      <td>[0.03612189739942551, 0.05468646064400673, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21804</th>\n",
       "      <td>this guy is a dirty jew213.152.254.36</td>\n",
       "      <td>identity_hate</td>\n",
       "      <td>this guy is a dirty jew</td>\n",
       "      <td>this guy is a dirty jew , this is identity_hate</td>\n",
       "      <td>[[CLS], this, guy, is, a, dirty, jew, ,, this,...</td>\n",
       "      <td>[[tensor(101), tensor(2023), tensor(3124), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.9331061045328776, 0.1975609560807546, 0.583...</td>\n",
       "      <td>[0.1243976503610611, 0.19353410601615906, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15216</th>\n",
       "      <td>your boyfriend s a fucking faggot</td>\n",
       "      <td>homophobic</td>\n",
       "      <td>your boyfriend s a fucking faggot</td>\n",
       "      <td>your boyfriend s a fucking faggot, this is hom...</td>\n",
       "      <td>[[CLS], your, boyfriend, s, a, fucking, fa, ##...</td>\n",
       "      <td>[[tensor(101), tensor(2115), tensor(6898), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.2926476337015629, 0.347992368042469, 0.5345...</td>\n",
       "      <td>[0.028020694851875305, 0.05802285671234131, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69906</th>\n",
       "      <td>hey nazi punk i wiol troll your ass you think ...</td>\n",
       "      <td>slur</td>\n",
       "      <td>hey nazi punk i wiol troll your ass you think ...</td>\n",
       "      <td>hey nazi punk i wiol troll your ass you think ...</td>\n",
       "      <td>[[CLS], hey, nazi, punk, i, wi, ##ol, troll, y...</td>\n",
       "      <td>[[tensor(101), tensor(4931), tensor(6394), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.5055870562791824, 0.091542087495327, 0.6692...</td>\n",
       "      <td>[0.09904444962739944, 0.05369359999895096, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22026</th>\n",
       "      <td>You son of a bitch \\nI was the one who recogni...</td>\n",
       "      <td>identity_hate</td>\n",
       "      <td>you son of a bitch i was the one who recognize...</td>\n",
       "      <td>you son of a bitch i was the one who recognize...</td>\n",
       "      <td>[[CLS], you, son, of, a, bitch, i, was, the, o...</td>\n",
       "      <td>[[tensor(101), tensor(2017), tensor(2365), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.8317039310932159, 0.05167794662217299, 0.63...</td>\n",
       "      <td>[0.02894088625907898, -0.21220093965530396, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24738</th>\n",
       "      <td>Yeah, all you have to do is stop being such a ...</td>\n",
       "      <td>insult</td>\n",
       "      <td>yeah all you have to do is stop being such a b...</td>\n",
       "      <td>yeah all you have to do is stop being such a b...</td>\n",
       "      <td>[[CLS], yeah, all, you, have, to, do, is, stop...</td>\n",
       "      <td>[[tensor(101), tensor(3398), tensor(2035), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.19321022927761078, -0.2766666114330292, -0....</td>\n",
       "      <td>[0.03563609719276428, -0.02736884355545044, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53855</th>\n",
       "      <td>RT @spiffytwiley: @vintage_monroe_ bitch you g...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>rt monroe bitch you gorgeous</td>\n",
       "      <td>rt monroe bitch you gorgeous, this is offensive</td>\n",
       "      <td>[[CLS], rt, monroe, bitch, you, gorgeous, ,, t...</td>\n",
       "      <td>[[tensor(101), tensor(19387), tensor(9747), te...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.23898236453533173, 0.9837923049926758, 0.3...</td>\n",
       "      <td>[-0.025485828518867493, -0.03901668265461922, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74112</th>\n",
       "      <td>&lt;USER&gt; shana i'm shanna with two n's that bitch</td>\n",
       "      <td>vulgar</td>\n",
       "      <td>shana i m shanna with two n s that bitch</td>\n",
       "      <td>shana i m shanna with two n s that bitch, thi...</td>\n",
       "      <td>[[CLS], shan, ##a, i, m, shan, ##na, with, two...</td>\n",
       "      <td>[[tensor(101), tensor(17137), tensor(2050), te...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.36011314392089844, -0.5255359411239624, -0...</td>\n",
       "      <td>[0.10889077931642532, 0.15816475450992584, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>RT @Ironyisfunny8: The Jews of #Iraq lost ever...</td>\n",
       "      <td>harassment</td>\n",
       "      <td>rt the jews of iraq lost everything thanks to ...</td>\n",
       "      <td>rt the jews of iraq lost everything thanks to ...</td>\n",
       "      <td>[[CLS], rt, the, jews, of, iraq, lost, everyth...</td>\n",
       "      <td>[[tensor(101), tensor(19387), tensor(1996), te...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.059251975268125534, -0.7964531183242798, -...</td>\n",
       "      <td>[-0.13810834288597107, -0.015917573124170303, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text          label  \\\n",
       "1129   all trump supporters are worthless pieces of s...          abuse   \n",
       "37731           Fuck you and Fuck your mom. And her dog.        obscene   \n",
       "21804              this guy is a dirty jew213.152.254.36  identity_hate   \n",
       "15216                  your boyfriend s a fucking faggot     homophobic   \n",
       "69906  hey nazi punk i wiol troll your ass you think ...           slur   \n",
       "...                                                  ...            ...   \n",
       "22026  You son of a bitch \\nI was the one who recogni...  identity_hate   \n",
       "24738  Yeah, all you have to do is stop being such a ...         insult   \n",
       "53855  RT @spiffytwiley: @vintage_monroe_ bitch you g...      offensive   \n",
       "74112    <USER> shana i'm shanna with two n's that bitch         vulgar   \n",
       "10271  RT @Ironyisfunny8: The Jews of #Iraq lost ever...     harassment   \n",
       "\n",
       "                                       preprocessed_text  \\\n",
       "1129   all trump supporters are worthless pieces of s...   \n",
       "37731            fuck you and fuck your mom and her dog    \n",
       "21804                           this guy is a dirty jew    \n",
       "15216                  your boyfriend s a fucking faggot   \n",
       "69906  hey nazi punk i wiol troll your ass you think ...   \n",
       "...                                                  ...   \n",
       "22026  you son of a bitch i was the one who recognize...   \n",
       "24738  yeah all you have to do is stop being such a b...   \n",
       "53855                       rt monroe bitch you gorgeous   \n",
       "74112           shana i m shanna with two n s that bitch   \n",
       "10271  rt the jews of iraq lost everything thanks to ...   \n",
       "\n",
       "                                                appended  \\\n",
       "1129   all trump supporters are worthless pieces of s...   \n",
       "37731  fuck you and fuck your mom and her dog , this ...   \n",
       "21804    this guy is a dirty jew , this is identity_hate   \n",
       "15216  your boyfriend s a fucking faggot, this is hom...   \n",
       "69906  hey nazi punk i wiol troll your ass you think ...   \n",
       "...                                                  ...   \n",
       "22026  you son of a bitch i was the one who recognize...   \n",
       "24738  yeah all you have to do is stop being such a b...   \n",
       "53855    rt monroe bitch you gorgeous, this is offensive   \n",
       "74112   shana i m shanna with two n s that bitch, thi...   \n",
       "10271  rt the jews of iraq lost everything thanks to ...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "1129   [[CLS], all, trump, supporters, are, worthless...   \n",
       "37731  [[CLS], fuck, you, and, fuck, your, mom, and, ...   \n",
       "21804  [[CLS], this, guy, is, a, dirty, jew, ,, this,...   \n",
       "15216  [[CLS], your, boyfriend, s, a, fucking, fa, ##...   \n",
       "69906  [[CLS], hey, nazi, punk, i, wi, ##ol, troll, y...   \n",
       "...                                                  ...   \n",
       "22026  [[CLS], you, son, of, a, bitch, i, was, the, o...   \n",
       "24738  [[CLS], yeah, all, you, have, to, do, is, stop...   \n",
       "53855  [[CLS], rt, monroe, bitch, you, gorgeous, ,, t...   \n",
       "74112  [[CLS], shan, ##a, i, m, shan, ##na, with, two...   \n",
       "10271  [[CLS], rt, the, jews, of, iraq, lost, everyth...   \n",
       "\n",
       "                                           tokens_tensor  \\\n",
       "1129   [[tensor(101), tensor(2035), tensor(8398), ten...   \n",
       "37731  [[tensor(101), tensor(6616), tensor(2017), ten...   \n",
       "21804  [[tensor(101), tensor(2023), tensor(3124), ten...   \n",
       "15216  [[tensor(101), tensor(2115), tensor(6898), ten...   \n",
       "69906  [[tensor(101), tensor(4931), tensor(6394), ten...   \n",
       "...                                                  ...   \n",
       "22026  [[tensor(101), tensor(2017), tensor(2365), ten...   \n",
       "24738  [[tensor(101), tensor(3398), tensor(2035), ten...   \n",
       "53855  [[tensor(101), tensor(19387), tensor(9747), te...   \n",
       "74112  [[tensor(101), tensor(17137), tensor(2050), te...   \n",
       "10271  [[tensor(101), tensor(19387), tensor(1996), te...   \n",
       "\n",
       "                                        segments_tensors  \\\n",
       "1129   [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "37731  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "21804  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "15216  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "69906  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "...                                                  ...   \n",
       "22026  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "24738  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "53855  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "74112  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "10271  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "\n",
       "                                          bert_emmbeding  \\\n",
       "1129   [0.7415121793746948, 0.31940165162086487, 0.24...   \n",
       "37731  [0.3871220350265503, 0.4998416602611542, 0.015...   \n",
       "21804  [0.9331061045328776, 0.1975609560807546, 0.583...   \n",
       "15216  [0.2926476337015629, 0.347992368042469, 0.5345...   \n",
       "69906  [0.5055870562791824, 0.091542087495327, 0.6692...   \n",
       "...                                                  ...   \n",
       "22026  [0.8317039310932159, 0.05167794662217299, 0.63...   \n",
       "24738  [0.19321022927761078, -0.2766666114330292, -0....   \n",
       "53855  [-0.23898236453533173, 0.9837923049926758, 0.3...   \n",
       "74112  [-0.36011314392089844, -0.5255359411239624, -0...   \n",
       "10271  [-0.059251975268125534, -0.7964531183242798, -...   \n",
       "\n",
       "                                       bert_sentence_emm  \n",
       "1129   [0.04157046601176262, 0.04623369872570038, 0.0...  \n",
       "37731  [0.03612189739942551, 0.05468646064400673, 0.0...  \n",
       "21804  [0.1243976503610611, 0.19353410601615906, 0.08...  \n",
       "15216  [0.028020694851875305, 0.05802285671234131, 0....  \n",
       "69906  [0.09904444962739944, 0.05369359999895096, 0.0...  \n",
       "...                                                  ...  \n",
       "22026  [0.02894088625907898, -0.21220093965530396, -0...  \n",
       "24738  [0.03563609719276428, -0.02736884355545044, 0....  \n",
       "53855  [-0.025485828518867493, -0.03901668265461922, ...  \n",
       "74112  [0.10889077931642532, 0.15816475450992584, 0.0...  \n",
       "10271  [-0.13810834288597107, -0.015917573124170303, ...  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = df.drop(['tokens_tensor', 'segments_tensors', 'tokenized_text', 'text'], axis = 1)\n",
    "dfs = df.sample(10000)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e403963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 70, 1024), dtype=float32, numpy=\n",
       "array([[[-1.3958732 , -0.47634852, -0.42686164, ...,  0.36568496,\n",
       "          0.49042425,  0.2354556 ],\n",
       "        [ 0.4454992 ,  0.2643395 , -0.31102926, ...,  0.7670144 ,\n",
       "          0.26002738,  0.53752214],\n",
       "        [-0.6416162 , -0.8899971 ,  0.257272  , ..., -0.10393001,\n",
       "          0.38832465,  0.10515966],\n",
       "        ...,\n",
       "        [ 0.21973157,  0.16580486, -0.19386515, ..., -0.20244905,\n",
       "          1.0564073 , -0.2821011 ],\n",
       "        [ 0.22624248, -0.9736665 ,  0.12904175, ..., -0.46746704,\n",
       "          0.19730377,  0.03653371],\n",
       "        [-0.28542387, -0.1519949 , -0.02292022, ...,  0.10154832,\n",
       "         -0.04778115,  0.05461949]],\n",
       "\n",
       "       [[-1.0019374 ,  0.06765231,  0.02528628, ...,  0.07119475,\n",
       "          0.6233085 , -0.04681645],\n",
       "        [-0.6316018 , -0.16417497, -0.40858147, ...,  0.54096615,\n",
       "         -0.05943623, -0.41254646],\n",
       "        [ 0.24779448, -0.18716079, -0.11601172, ...,  0.23703824,\n",
       "          0.12029931, -0.11658347],\n",
       "        ...,\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422]],\n",
       "\n",
       "       [[-0.12058148,  0.35792196,  0.26932147, ..., -0.42125773,\n",
       "          0.55097246, -0.39850533],\n",
       "        [-0.32921067,  0.4173649 ,  0.1808441 , ..., -0.5620682 ,\n",
       "          0.8267255 , -0.1937739 ],\n",
       "        [ 0.3416338 ,  0.24568418, -0.08515576, ..., -0.1100444 ,\n",
       "          0.14861599, -0.32320455],\n",
       "        ...,\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.3156882 , -0.52360815,  0.2647127 , ..., -0.15214339,\n",
       "          0.02531537,  0.44089705],\n",
       "        [ 0.6559384 , -0.008431  ,  0.04770572, ...,  0.2354434 ,\n",
       "         -0.1732749 ,  0.38349825],\n",
       "        [-0.27642208, -0.43018773,  0.33148924, ..., -0.11124672,\n",
       "          0.16927582, -0.03857329],\n",
       "        ...,\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422]],\n",
       "\n",
       "       [[ 0.31568822, -0.5236081 ,  0.2647128 , ..., -0.4087599 ,\n",
       "          0.68891776,  0.12097776],\n",
       "        [ 0.20470527,  0.45553425,  0.20764425, ..., -0.8599808 ,\n",
       "          0.52845997,  0.04012597],\n",
       "        [ 0.29748088,  0.17687152, -0.15502122, ...,  0.4941826 ,\n",
       "          0.25577873, -0.36938575],\n",
       "        ...,\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422]],\n",
       "\n",
       "       [[ 0.0246214 , -0.06375615,  0.1562462 , ...,  0.0685423 ,\n",
       "          0.2586925 , -0.11710468],\n",
       "        [ 0.1880428 , -0.15395296, -0.0641397 , ..., -0.4485902 ,\n",
       "          0.36612123,  0.40538558],\n",
       "        [-0.15040655, -0.04787906,  0.10585555, ...,  0.44765252,\n",
       "          0.13262759, -0.18715869],\n",
       "        ...,\n",
       "        [-0.02840841, -0.04353216,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.0284084 , -0.04353215,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422],\n",
       "        [-0.0284084 , -0.04353215,  0.04130162, ...,  0.02583168,\n",
       "         -0.01429836, -0.01650422]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst =  df['preprocessed_text'].tolist()\n",
    "lst2 = df['appended'].tolist()\n",
    "embeddings_words = elmo(tf.constant(lst))[\"elmo\"]\n",
    "embeddings_sent = elmo(tf.constant(lst2))[\"default\"]\n",
    "embeddings_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_words.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf97149",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_sent.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8406a97",
   "metadata": {},
   "source": [
    "Just for last word (this is racist etc) embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['elmo_sentence'] = embeddings_sent.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fae3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idx'] = df.progress_apply(lambda x: len(x['appended'].split()), axis = 1)\n",
    "df['shape'] = df.progress_apply(lambda x: len(x['elmo_word']), axis = 1)\n",
    "df['idx'] = df.progress_apply(lambda x: min(x['idx'], x['shape']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd429c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8384c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['elmo_word'] = embeddings_words.numpy().tolist()\n",
    "df['elmo_word'] = df.progress_apply(lambda x: x['elmo_word'][x['idx']-1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['elmo_words', 'idx', 'shape'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b094e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('elmobert.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
